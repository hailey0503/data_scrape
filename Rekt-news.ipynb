{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web page: https://rekt.news/ \n",
    "# tool: beautifulSoup for scraping and pandas for data processing\n",
    "\n",
    "\n",
    "# url request and download & save web page as files\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import csv\n",
    "import time\n",
    "\n",
    "## Setup chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\") # Ensure GUI is off\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'\n",
    "chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "\n",
    "# Set path to chromedriver as per your configuration\n",
    "homedir = os.path.expanduser(\"~\")\n",
    "webdriver_service = Service(f\"/usr/local/bin/chromedriver\")\n",
    "\n",
    "# Choose Chrome Browser\n",
    "browser = webdriver.Chrome(service=webdriver_service, options=chrome_options)\n",
    "\n",
    "# getting max page\n",
    "\n",
    "def maxPage(soup):\n",
    "    #find max page\n",
    "    page_info = soup.select_one('.page-number')\n",
    "    # page 0 of max_page\n",
    "    max_page = page_info.get_text().split()[3]\n",
    "    max_page = int(max_page)\n",
    "    return max_page\n",
    "\n",
    "\n",
    "#  extract articles from the page. parse and store into a list.\n",
    "#  save html to file\n",
    "\n",
    "def parseArticles(article_paths):\n",
    "\n",
    "\n",
    "    for i in range(len(article_paths)):\n",
    "        url = \"https://rekt.news/\"+ article_paths[i]\n",
    "      \n",
    "        article_data = requests.get(url)\n",
    "        soup = BeautifulSoup(article_data.text, 'html.parser')\n",
    "        main = soup.select_one('main.content')\n",
    "        post_header = main.select_one('.post-header')\n",
    "        title = post_header.select_one('.post-title')\n",
    "        date = post_header.select_one('.post-meta time')\n",
    "\n",
    "        title = title.get_text()\n",
    "        date = date.get_text()\n",
    "        content_children = main.select_one(\".post-content\").children\n",
    "        content_texts = []\n",
    "        for child in content_children:\n",
    "            content_texts.append(child.get_text())\n",
    "\n",
    "        content = \"\\n\".join(content_texts)\n",
    "\n",
    "        post_data.append([title, date, content])\n",
    "        \n",
    "        # create a file and save html\n",
    "        path = article_paths[i].replace('/','')\n",
    "        file = os.path.join(download_path, f'{path}.html') \n",
    "\n",
    "        with open(file, 'w') as f:\n",
    "            f.write(article_data.text)   \n",
    "\n",
    "# create a csv file and write data on it\n",
    "\n",
    "def createCSV(data):\n",
    "    fields = [\"title\", \"date\", \"content\"]\n",
    "    # open the file in the write mode\n",
    "    filename = \"rekted_news.csv\"\n",
    "    with open(filename, 'w') as csvfile:\n",
    "        # create the csv writer\n",
    "        csvwriter = csv.writer(csvfile) \n",
    "        csvwriter.writerow(fields) \n",
    "        # writing the data rows \n",
    "        csvwriter.writerows(data)\n",
    "            \n",
    "\n",
    "\n",
    "url = 'https://rekt.news/'\n",
    "\n",
    "data = requests.get(url)\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "# find max page\n",
    "max_page = maxPage(soup) + 1\n",
    "\n",
    "# create path\n",
    "homedir = os.path.expanduser(\"~\")\n",
    "download_path = f'{homedir}/Desktop/Certik'\n",
    "if not os.path.exists(download_path):\n",
    "    os.makedirs(download_path)\n",
    "file = os.path.join(download_path, \"rektnews.html\") \n",
    "\n",
    "post_data = []\n",
    "\n",
    "# create page url and download each page's html to files\n",
    "for i in range(max_page):\n",
    "    # create page url\n",
    "\n",
    "    page_url = url + \"?page=\" + str(i)\n",
    "    browser.get(page_url)\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    \n",
    "    # getting each article' url\n",
    "    articles = soup.select('#grid article')\n",
    "    my_urls = []\n",
    "\n",
    "    for article in articles:\n",
    "        a = article.select('a')[0]\n",
    "        my_urls.append(a['href'])\n",
    "    #  extract articles from the page. parse and store into a data_list.    \n",
    "    parseArticles(my_urls)\n",
    "\n",
    "# create a csv file and write data\n",
    "createCSV(post_data)\n",
    "\n",
    "browser.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
